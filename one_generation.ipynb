{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from uncertainty.utils import utils\n",
    "import hashlib\n",
    "import random\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "import torch._dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25d991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordsCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_strings, tokenizer, input_ids):\n",
    "        super().__init__()\n",
    "        self.stop_strings = stop_strings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_len = input_ids.shape[1]\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the newly generated tokens\n",
    "        decoded = self.tokenizer.batch_decode(input_ids[:, self.input_len:], skip_special_tokens=True)\n",
    "        for text in decoded:\n",
    "            if any(stop_string in text for stop_string in self.stop_strings):\n",
    "                print(\"STOPPING CRITERIA EVOKED\")\n",
    "                return True  # Stop generation\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00412401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d01ff8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254d8fd8433045af92eb6c616170cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e37457c89e46cbb30ba1efc46361bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc38a0880a94150b5bf3ee50ee11a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a74c87b99f473da9a9d38cb4f37348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb8c9bf92f447a5a1557bb62d2801d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa286d2fe1b4471a8866e2da25635c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d14368c6699411599c672ebf1871d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa8dd55aa914ad28d756f0a4e7fe035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c51ef494adc467b9693bd2dd0a7dda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eda9b5dfed454b8c1ccb85c4a7c153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143d554d2de9406fa52a3ed091b98955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8d51cbff074f0e95d37801a3a6a0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map='auto', token_type_ids=None, clean_up_tokenization_spaces=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "token_limit = 8192\n",
    "    \n",
    "def batch_predict(self, prompts, temperature, return_latent=False, batch_size=10):\n",
    "    \"\"\"Generate answers for a batch of prompts and return text, log-likelihoods, and embeddings if requested.\"\"\"\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    results = []\n",
    "\n",
    "    for batch_start in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[batch_start:batch_start + batch_size]\n",
    "\n",
    "       # Tokenize batch prompts\n",
    "        encoded = self.tokenizer(\n",
    "                batch_prompts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.token_limit - self.max_new_tokens,\n",
    "                return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Define your stop strings\n",
    "        # stop_strings = [\"\\n\\n\", \"Question:\", \"Context:\", \"Now, provide\", \"Use exactly\", \"Evaluation\"]\n",
    "        stop_strings = []\n",
    "\n",
    "        # Build stopping criteria\n",
    "        stop_criteria = StoppingCriteriaList([\n",
    "                StopWordsCriteria(stop_strings, self.tokenizer, encoded[\"input_ids\"])\n",
    "            ])\n",
    "\n",
    "        allowed_keys = {\"input_ids\", \"attention_mask\"}\n",
    "        safe_encoded = {k: v for k, v in encoded.items() if k in allowed_keys}\n",
    "\n",
    "        # Now call generate with stopping_criteria\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                    **safe_encoded,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    output_hidden_states=True,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    stopping_criteria=stop_criteria,\n",
    "                    top_p = 0.9,\n",
    "                    top_k = 50\n",
    "                )\n",
    "            \n",
    "            eos_token_id = self.tokenizer.eos_token_id\n",
    "            for i, sequence in enumerate(outputs.sequences):\n",
    "                if eos_token_id in sequence:\n",
    "                    print(f\"[EOS Detected] Sample {i} includes eos_token_id ({eos_token_id})\")\n",
    "                    # Optional: where in the sequence\n",
    "                    eos_positions = (sequence == eos_token_id).nonzero(as_tuple=True)[0].tolist()\n",
    "                    print(f\" → EOS token found at indices: {eos_positions}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"[EOS Not Found] Sample {i} did not include eos_token_id\")\n",
    "\n",
    "            # ---- FIXED SECTION ----\n",
    "            # last layer → shape (batch, seq_len, hidden_size)\n",
    "            \n",
    "            hidden_states      = outputs.hidden_states[0][-1][0,0]\n",
    "            last_token_embedding = hidden_states.cpu()\n",
    "            \n",
    "            for i, prompt in enumerate(batch_prompts):\n",
    "                full_answer   = self.tokenizer.decode(outputs.sequences[i], skip_special_tokens=True)\n",
    "                sliced_answer = full_answer[len(prompt):].strip()\n",
    "\n",
    "                # Prompt / generation lengths\n",
    "                token_stop_index = self.tokenizer(full_answer, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "                n_input_token    = encoded[\"input_ids\"][i].ne(self.tokenizer.pad_token_id).sum().item()\n",
    "                n_generated      = token_stop_index - n_input_token or 1\n",
    "\n",
    "                # -----------------------\n",
    "\n",
    "                # Log-likelihoods for generated tokens\n",
    "                transition_scores = self.model.compute_transition_scores(\n",
    "                    outputs.sequences, outputs.scores, normalize_logits=True\n",
    "                )\n",
    "                log_likelihoods = [score.item() for score in transition_scores[i][:n_generated]]\n",
    "\n",
    "                if return_latent:\n",
    "                    results.append((sliced_answer, log_likelihoods, (last_token_embedding, None, None)))\n",
    "                else:\n",
    "                    results.append((sliced_answer, log_likelihoods, None))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be3d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
