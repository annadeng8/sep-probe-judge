{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47e0af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from uncertainty.utils import utils\n",
    "import hashlib\n",
    "import random\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList\n",
    "import torch._dynamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopWordsCriteria(StoppingCriteria):\n",
    "    def __init__(self, stop_strings, tokenizer, input_ids):\n",
    "        super().__init__()\n",
    "        self.stop_strings = stop_strings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_len = input_ids.shape[1]\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Decode only the newly generated tokens\n",
    "        decoded = self.tokenizer.batch_decode(input_ids[:, self.input_len:], skip_special_tokens=True)\n",
    "        for text in decoded:\n",
    "            if any(stop_string in text for stop_string in self.stop_strings):\n",
    "                print(\"STOPPING CRITERIA EVOKED\")\n",
    "                return True  # Stop generation\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, device_map='auto', token_type_ids=None, clean_up_tokenization_spaces=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "token_limit = 8192\n",
    "    \n",
    "def batch_predict(self, prompts, temperature, return_latent=False, batch_size=10):\n",
    "    \"\"\"Generate answers for a batch of prompts and return text, log-likelihoods, and embeddings if requested.\"\"\"\n",
    "    import torch\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    results = []\n",
    "\n",
    "    for batch_start in range(0, len(prompts), batch_size):\n",
    "        batch_prompts = prompts[batch_start:batch_start + batch_size]\n",
    "\n",
    "       # Tokenize batch prompts\n",
    "        encoded = self.tokenizer(\n",
    "                batch_prompts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.token_limit - self.max_new_tokens,\n",
    "                return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Define your stop strings\n",
    "        # stop_strings = [\"\\n\\n\", \"Question:\", \"Context:\", \"Now, provide\", \"Use exactly\", \"Evaluation\"]\n",
    "        stop_strings = []\n",
    "\n",
    "        # Build stopping criteria\n",
    "        stop_criteria = StoppingCriteriaList([\n",
    "                StopWordsCriteria(stop_strings, self.tokenizer, encoded[\"input_ids\"])\n",
    "            ])\n",
    "\n",
    "        allowed_keys = {\"input_ids\", \"attention_mask\"}\n",
    "        safe_encoded = {k: v for k, v in encoded.items() if k in allowed_keys}\n",
    "\n",
    "        # Now call generate with stopping_criteria\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                    **safe_encoded,\n",
    "                    max_new_tokens=self.max_new_tokens,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores=True,\n",
    "                    output_hidden_states=True,\n",
    "                    temperature=temperature,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    stopping_criteria=stop_criteria,\n",
    "                    top_p = 0.9,\n",
    "                    top_k = 50\n",
    "                )\n",
    "            \n",
    "            eos_token_id = self.tokenizer.eos_token_id\n",
    "            for i, sequence in enumerate(outputs.sequences):\n",
    "                if eos_token_id in sequence:\n",
    "                    print(f\"[EOS Detected] Sample {i} includes eos_token_id ({eos_token_id})\")\n",
    "                    # Optional: where in the sequence\n",
    "                    eos_positions = (sequence == eos_token_id).nonzero(as_tuple=True)[0].tolist()\n",
    "                    print(f\" → EOS token found at indices: {eos_positions}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"[EOS Not Found] Sample {i} did not include eos_token_id\")\n",
    "\n",
    "            # ---- FIXED SECTION ----\n",
    "            # last layer → shape (batch, seq_len, hidden_size)\n",
    "            \n",
    "            hidden_states      = outputs.hidden_states[0][-1][0,0]\n",
    "            last_token_embedding = hidden_states.cpu()\n",
    "            \n",
    "            for i, prompt in enumerate(batch_prompts):\n",
    "                full_answer   = self.tokenizer.decode(outputs.sequences[i], skip_special_tokens=True)\n",
    "                sliced_answer = full_answer[len(prompt):].strip()\n",
    "\n",
    "                # Prompt / generation lengths\n",
    "                token_stop_index = self.tokenizer(full_answer, return_tensors=\"pt\")[\"input_ids\"].shape[1]\n",
    "                n_input_token    = encoded[\"input_ids\"][i].ne(self.tokenizer.pad_token_id).sum().item()\n",
    "                n_generated      = token_stop_index - n_input_token or 1\n",
    "\n",
    "                # -----------------------\n",
    "\n",
    "                # Log-likelihoods for generated tokens\n",
    "                transition_scores = self.model.compute_transition_scores(\n",
    "                    outputs.sequences, outputs.scores, normalize_logits=True\n",
    "                )\n",
    "                log_likelihoods = [score.item() for score in transition_scores[i][:n_generated]]\n",
    "\n",
    "                if return_latent:\n",
    "                    results.append((sliced_answer, log_likelihoods, (last_token_embedding, None, None)))\n",
    "                else:\n",
    "                    results.append((sliced_answer, log_likelihoods, None))\n",
    "\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
